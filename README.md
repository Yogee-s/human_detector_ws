# Human & Teleco Robot Detection

**Osaka University Frontier Programme: Social Robotics Group (Yoshikawa Lab)**
---
Realâ€time 2D detection of **humans** and a custom **Teleco robot**, plus 3D distance/position estimation using an Intel RealSense camera.

---

## ğŸ” Project Overview

* **Context:** Developed at Osaka Univ. Frontier Programme (Yoshikawa Lab) for human/Teleco detection.
* **Hardware:** Intel RealSense D435 for RGB + depth streams.
* **Model:** Ultralytics YOLO (`yolo11n.pt`) customâ€trained on COCOâ€person + Teleco datasets.
* **ROS Integration:** Wraps detection into ROSÂ 2 nodes; optionally publishes 3D point clouds.

---

## ğŸ“‚ Repository Layout

```text
HUMAN_DETECTOR_WS/
â”œâ”€â”€ data_raw/            # raw exports from RealSense & ROS bags
â”‚   â”œâ”€â”€ converted_mp4_videos/
â”‚   â””â”€â”€ labelled_data/   # CVAT/Roboflow annotations
â”œâ”€â”€ data_training/       # train/val splits & YAML config
â”‚   â”œâ”€â”€ data_person/     # COCOâ€‘person subset
â”‚   â”œâ”€â”€ data_teleco/     # Teleco frames & labels
â”‚   â””â”€â”€ data.yaml        # nc:2, names: ['person','teleco']
â”œâ”€â”€ detection_scripts/   # standalone Python demos
â”‚   â”œâ”€â”€ laptop_human_publisher.py
â”‚   â””â”€â”€ simple_detector.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ human_detector/  # ROSÂ 2 detection package
â”‚   â”œâ”€â”€ yolo_ros/        # upstream ROS wrapper (mgonzs13/
â””â”€â”€ README.md            # this file
```

---

## âš™ï¸ Dependencies & Installation

1. **ROSÂ 2 Humble** (source install)
2. **Python 3.10+** with:

   ```bash
   pip install ultralytics opencv-python pyrealsense2 pyyaml
   ```
3. **Build workspace**

   ```bash
   cd ~/Desktop/human_detector_ws
   source /opt/ros/humble/setup.bash
   colcon build --symlink-install
   source install/setup.bash
   ```

---

## ğŸ›  Data Processing & Annotation

1. **Record raw data** via RealSense / ROSâ€bag.
2. **Convert & extract frames** using `data_raw/download_human_data.py` or `helper_scripts.ipynb`.
3. **Label** in CVAT (seeâ€¯[https://docs.cvat.ai/docs/administration/basics/installation/](https://docs.cvat.ai/docs/administration/basics/installation/)).
4. **Prepare** `data_training/` splits and `data.yaml` with helper notebooks.

---

## ğŸ“ Model Training

### Headless (CLI)

```bash
python3 train.py \
  --data data_training/data.yaml \
  --model yolov11n.pt \
  --epochs 100 \
  --imgsz 640 \
  --batch 16
```

### Interactive (Notebook)

Open **train.ipynb** and follow cells to train & visualize metrics.

---

## ğŸš€ Inference & ROS 2 Usage

### 1) 2D Detection only

```bash
ros2 launch human_detector human_detector_launch.py
```

### 2) 2D + 3D Distance (with yolo\_ros)

```bash
ros2 launch yolo_bringup yolov11.launch.py use_3d:=True
```

### 3) Standalone Python demos

```bash
python3 detection_scripts/simple_detector.py
python3 detection_scripts/laptop_human_publisher.py
```

---

## ğŸ”— Upstream & References

* **CVAT** for annotation: [https://docs.cvat.ai/docs/administration/basics/installation/](https://docs.cvat.ai/docs/administration/basics/installation/)
* **YOLOâ€ROS** wrapper: [https://github.com/mgonzs13/yolo\_ros](https://github.com/mgonzs13/yolo_ros)
* **COCO** open-source person dataset: [https://docs.ultralytics.com/datasets/detect/coco/](https://docs.ultralytics.com/datasets/detect/coco/)

---

