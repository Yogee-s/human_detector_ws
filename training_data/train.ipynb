{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba0137",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'\u001b[31m\u001b[1mhyp\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=/run/user/1000/jupyter/runtime/kernel-v3426a4afb468cfa9a7934d7780764969ed7794264.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'pose', 'detect', 'segment', 'classify', 'obb'})\n                MODE (required) is one of frozenset({'track', 'train', 'predict', 'benchmark', 'val', 'export'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n     (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3457\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"/tmp/ipykernel_3559/2092987505.py\"\u001b[0m, line \u001b[1;32m22\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    results = model.train(\n",
      "  File \u001b[1;32m\"/home/yogee/.local/lib/python3.10/site-packages/ultralytics/engine/model.py\"\u001b[0m, line \u001b[1;32m790\u001b[0m, in \u001b[1;35mtrain\u001b[0m\n    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "  File \u001b[1;32m\"/home/yogee/.local/lib/python3.10/site-packages/ultralytics/engine/trainer.py\"\u001b[0m, line \u001b[1;32m103\u001b[0m, in \u001b[1;35m__init__\u001b[0m\n    self.args = get_cfg(cfg, overrides)\n",
      "  File \u001b[1;32m\"/home/yogee/.local/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\"\u001b[0m, line \u001b[1;32m306\u001b[0m, in \u001b[1;35mget_cfg\u001b[0m\n    check_dict_alignment(cfg, overrides)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/yogee/.local/lib/python3.10/site-packages/ultralytics/cfg/__init__.py\"\u001b[0;36m, line \u001b[0;32m496\u001b[0;36m, in \u001b[0;35mcheck_dict_alignment\u001b[0;36m\u001b[0m\n\u001b[0;31m    raise SyntaxError(string + CLI_HELP_MSG) from e\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '\u001b[31m\u001b[1mhyp\u001b[0m' is not a valid YOLO argument. \n\n    Arguments received: ['yolo', '--f=/run/user/1000/jupyter/runtime/kernel-v3426a4afb468cfa9a7934d7780764969ed7794264.json']. Ultralytics 'yolo' commands use the following syntax:\n\n        yolo TASK MODE ARGS\n\n        Where   TASK (optional) is one of frozenset({'pose', 'detect', 'segment', 'classify', 'obb'})\n                MODE (required) is one of frozenset({'track', 'train', 'predict', 'benchmark', 'val', 'export'})\n                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n\n    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n        yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01\n\n    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n        yolo predict model=yolo11n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n\n    3. Val a pretrained detection model at batch-size 1 and image size 640:\n        yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n\n    4. Export a YOLO11n classification model to ONNX format at image size 224 by 128 (no TASK required)\n        yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128\n\n    5. Ultralytics solutions usage\n        yolo solutions count or in ['crop', 'blur', 'workout', 'heatmap', 'isegment', 'visioneye', 'speed', 'queue', 'analytics', 'inference', 'trackzone'] source=\"path/to/video.mp4\"\n\n    6. Run special commands:\n        yolo help\n        yolo checks\n        yolo version\n        yolo settings\n        yolo copy-cfg\n        yolo cfg\n        yolo solutions help\n\n    Docs: https://docs.ultralytics.com\n    Solutions: https://docs.ultralytics.com/solutions/\n    Community: https://community.ultralytics.com\n    GitHub: https://github.com/ultralytics/ultralytics\n    \n"
     ]
    }
   ],
   "source": [
    "# YOLOv8 Training Script\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load a pretrained YOLOv8n variant (custom lightweight model)\n",
    "model = YOLO('../src/human_detector/models/yolov11n.pt')\n",
    "\n",
    "\n",
    "# Training with simple parameters\n",
    "#    - data: path to your data.yaml\n",
    "#    - epochs, imgsz, project, name: same CLI args\n",
    "# results = model.train(\n",
    "#     data    = 'data.yaml',\n",
    "#     epochs  = 1,\n",
    "#     imgsz   = 640,\n",
    "#     project = 'human_robot',\n",
    "#     name    = 'exp1'\n",
    "# )\n",
    "\n",
    "# Low-light training hyperparameters as a Python dictionary\n",
    "custom_hyp = {\n",
    "    'epochs': 120,\n",
    "    'patience': 20,\n",
    "    'imgsz': 416,\n",
    "    'batch': 2,\n",
    "    'device': 0,\n",
    "    'half': True,\n",
    "    'cache': False,\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.7,\n",
    "    'hsv_v': 0.4,     # Brightness variation for low-light\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'fliplr': 0.5,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.0,\n",
    "    'copy_paste': 0.0\n",
    "}\n",
    "\n",
    "# Train the model with custom augmentation settings and longer epochs\n",
    "results = model.train(\n",
    "    data='data.yaml',\n",
    "    overrides=custom_hyp,\n",
    "    project='human_robot',\n",
    "    name='exp_lowlight'\n",
    ")\n",
    "\n",
    "# Save trained model path\n",
    "save_dir = str(results.save_dir)\n",
    "best_pt = os.path.join(save_dir, 'weights', 'best.pt')\n",
    "print(f\"✅  Trained checkpoint saved to: {best_pt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb40d1",
   "metadata": {},
   "source": [
    "# 🧠 YOLOv8 Training Metrics Cheatsheet\n",
    "\n",
    "## 🔢 Loss Curves\n",
    "\n",
    "| Metric            | Meaning                                                | Goal / Good Trend                |\n",
    "|-------------------|--------------------------------------------------------|----------------------------------|\n",
    "| `train/box_loss`  | Bounding box regression loss (training)                | ↓ Steady decline                 |\n",
    "| `train/cls_loss`  | Classification loss (training)                         | ↓ Should fall and stabilize     |\n",
    "| `train/dfl_loss`  | Distribution Focal Loss (for bounding box quality)     | ↓ Smooth drop                   |\n",
    "| `val/box_loss`    | Box loss on validation set                             | ↓ Best indicator of generalization |\n",
    "| `val/cls_loss`    | Classification loss on validation                      | ↓ Falling → Good model fit      |\n",
    "| `val/dfl_loss`    | DFL loss on validation                                 | ↓ Matches train trend = no overfit |\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Detection Metrics\n",
    "\n",
    "| Metric                | Description                                                 | Goal / Interpretation           |\n",
    "|------------------------|-------------------------------------------------------------|----------------------------------|\n",
    "| `metrics/precision(B)` | % of correct detections out of all predictions              | ↑ High = few false positives     |\n",
    "| `metrics/recall(B)`    | % of actual objects detected                                | ↑ High = few missed detections   |\n",
    "| `metrics/mAP50(B)`     | Accuracy of boxes at 50% IoU threshold                     | ↑ Main object detection metric   |\n",
    "| `metrics/mAP50-95(B)`  | Average mAP over IoU 0.5 to 0.95 (stricter)                | ↑ Robust performance measure     |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Ideal Curve Patterns\n",
    "\n",
    "- **Losses**: should steadily decrease.\n",
    "- **Validation loss** rising while training loss decreases → possible **overfitting**.\n",
    "- **Precision & Recall**: should increase and plateau near 1.0.\n",
    "- **mAP**: aim for **>0.90 mAP50** and **>0.80 mAP50-95** for high-quality results.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Tips\n",
    "\n",
    "- 🧊 Use `half=True` for mixed precision training on supported GPUs.\n",
    "- 🧠 Watch `val/box_loss` and `mAP50-95` to catch overfitting early.\n",
    "- 🧪 If metrics plateau early, try increasing `epochs`, `imgsz`, or improving labels.\n",
    "- 💾 Check `runs/train/.../results.png` for curve visualization.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
